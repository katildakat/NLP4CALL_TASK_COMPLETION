{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laaCqpI2S_hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97426821-5fd9-4b6b-ab47-3125e5cb99b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q umap\n",
        "!pip install -q umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJMCWXGPPrc3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from scipy import stats\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "from sentence_transformers import models, SentenceTransformer\n",
        "from transformers import PreTrainedModel, BertConfig, BertModel, AutoTokenizer, AutoModel,AutoConfig, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "import eval_utils"
      ],
      "metadata": {
        "id": "XaXiNSShTIiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassificationMeanPooling(PreTrainedModel):\n",
        "  def __init__(self, bert_model, num_labels):\n",
        "      super().__init__(bert_model.config)\n",
        "      self.bert = bert_model\n",
        "      self.num_labels = num_labels\n",
        "      self.config.num_labels = num_labels\n",
        "      self.dropout = torch.nn.Dropout(0.1)\n",
        "      self.classifier = torch.nn.Linear(bert_model.config.hidden_size, num_labels)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "      outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "      last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "      input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "      sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "      sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "      mean_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "      pooled_output = self.dropout(mean_embeddings)\n",
        "      logits = self.classifier(pooled_output)\n",
        "\n",
        "      result = {\"logits\": logits}\n",
        "      if labels is not None:\n",
        "          loss_fct = torch.nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "          result[\"loss\"] = loss\n",
        "\n",
        "      return result\n",
        "    \n",
        "  @classmethod\n",
        "  def from_my_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
        "\n",
        "      print(\"loading pre-trained\")\n",
        "      # Load the configuration from the saved model\n",
        "      config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
        "      # Load the saved state_dict\n",
        "      state_dict = torch.load(f\"{pretrained_model_name_or_path}/pytorch_model.bin\")\n",
        "      # Derive num_labels from the state_dict\n",
        "      num_labels = state_dict[\"classifier.weight\"].size(0)\n",
        "      # Initialize the BERT model\n",
        "      bert_model = AutoModel.from_pretrained(pretrained_model_name_or_path, config=config)\n",
        "      # Create a new instance of the class\n",
        "      model = cls(bert_model, num_labels)\n",
        "      # Update the model weights\n",
        "      model.load_state_dict(state_dict)\n",
        "\n",
        "      return model"
      ],
      "metadata": {
        "id": "lWWkwbg6XRfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "YZVjQWzIG6hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWlzMdIH0AZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return bert_tokenizer(examples['clean_transcript'], padding=True, truncation=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDJplLlm0GdA"
      },
      "outputs": [],
      "source": [
        "def make_split_dataset(df, split, label_column):\n",
        "  df['label'] = df[label_column]-1\n",
        "  df = df[['sample','clean_transcript', 'label', 'split']].copy()\n",
        "  \n",
        "  test_df = df[df['split']==split].reset_index(drop=True)\n",
        "  dataset_test = Dataset.from_pandas(test_df).map(preprocess_function, batched=True)\n",
        "  \n",
        "  train_df = df[df['split']!=split]\n",
        "  train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "  dataset_train = Dataset.from_pandas(train_df).map(preprocess_function, batched=True)\n",
        "\n",
        "  new_dataset_train = dataset_train.remove_columns(['clean_transcript', 'split'])\n",
        "  new_dataset_test = dataset_test.remove_columns(['clean_transcript', 'split'])\n",
        "\n",
        "  return new_dataset_train, new_dataset_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_split_dataset_no_task(df, split, label_column, tasks_to_exclude):\n",
        "\n",
        "  train_df = df[df['split'] != split]\n",
        "  train_df = train_df[~train_df['task'].isin([1, 3, 13])]\n",
        "  train_df['label'] = train_df[label_column]-1\n",
        "  train_df = train_df[['sample','clean_transcript', 'label']]\n",
        "\n",
        "  df['label'] = df[label_column]-1\n",
        "  df = df[['sample','clean_transcript', 'label', 'split']].copy()\n",
        "  \n",
        "  test_df = df[df['split']==split].reset_index(drop=True)\n",
        "  dataset_test = Dataset.from_pandas(test_df).map(preprocess_function, batched=True)\n",
        "  \n",
        "  train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "  dataset_train = Dataset.from_pandas(train_df).map(preprocess_function, batched=True)\n",
        "\n",
        "  new_dataset_train = dataset_train.remove_columns(['clean_transcript'])\n",
        "  new_dataset_test = dataset_test.remove_columns(['clean_transcript', 'split'])\n",
        "\n",
        "  return new_dataset_train, new_dataset_test"
      ],
      "metadata": {
        "id": "YHFwwMkOkKWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DALUr45BLS7-"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"yo\",\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=1,\n",
        "    save_total_limit=1,\n",
        "    warmup_steps=0,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps = 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"drive/MyDrive/swedish_average.csv\")\n",
        "model_name = \"drive/MyDrive/SWE_TASK_MODELS_SIAM/epoch4_split{}\"\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"drive/MyDrive/SWE_TASK_MODELS_SIAM/epoch4_split0\")\n",
        "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
        "\n",
        "criterion_column = 'task_completion_mean' #'ta_facets'\n",
        "bin_column = 'ta_bins' #'ta_bins_r'\n",
        "folder_name = \"SWE_TA_MODELS_MEAN_TASK\"\n",
        "\n",
        "results_df = pd.DataFrame()\n",
        "all_true_cls = []\n",
        "all_true_reg = []\n",
        "all_samples = []\n",
        "\n",
        "for e in range(10):\n",
        "  print(\"epoch \"+str(e))\n",
        "  all_predictions_cls = []\n",
        "  all_predictions_knn = []\n",
        "  \n",
        "  for split in df['split'].unique():\n",
        "    print(\"----------------------------\")\n",
        "    print(\"split {}\".format(split))\n",
        "    \n",
        "    # make saving path\n",
        "    model_path = \"epoch{}_split{}\".format(e,split)\n",
        "    saving_path = \"drive/MyDrive/\"+folder_name+\"/\"+model_path\n",
        "    \n",
        "    train_df = df[df['split']!=split].reset_index(drop=True)\n",
        "    test_df = df[df['split']==split].reset_index(drop=True)\n",
        "    \n",
        "    dataset_train, dataset_test = make_split_dataset(df, split, bin_column)\n",
        "\n",
        "    if e == 0:\n",
        "      # add values to the df\n",
        "      split_true_cls = df[df['split']==split][bin_column].tolist()\n",
        "      split_true_reg = df[df['split']==split][criterion_column].tolist()\n",
        "      all_true_cls+=[x-1 for x in split_true_cls]\n",
        "      all_true_reg+=split_true_reg\n",
        "      \n",
        "      split_sample = df[df['split']==split]['sample'].tolist()\n",
        "      all_samples+=[s for s in split_sample]\n",
        "\n",
        "      # load untrained model\n",
        "      bert_model = BertModel.from_pretrained(model_name.format(split))\n",
        "      model = BertForSequenceClassificationMeanPooling(bert_model, num_labels=3)\n",
        "      model.to(device)\n",
        "\n",
        "    else:\n",
        "      # load trained in previous epoch\n",
        "      pre_model_path = \"drive/MyDrive/\"+folder_name+\"/epoch{}_split{}\".format(e-1,split)\n",
        "      model = BertForSequenceClassificationMeanPooling.from_my_pretrained(pre_model_path)\n",
        "      model.to(device)\n",
        "    \n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=dataset_train,\n",
        "      eval_dataset=dataset_test,\n",
        "      tokenizer=bert_tokenizer,\n",
        "      data_collator=data_collator)\n",
        "    \n",
        "    trainer.train()\n",
        "    model.save_pretrained(saving_path)\n",
        "    \n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset_test, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "    split_predictions = []\n",
        "    for batch in dataloader:\n",
        "      with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[\"logits\"]\n",
        "        batch_predictions = torch.argmax(logits, dim=1).tolist()\n",
        "        split_predictions+=batch_predictions\n",
        "    \n",
        "    all_predictions_cls+=split_predictions\n",
        "\n",
        "    mean_embed_dict = eval_utils.get_mean_dict(df['clean_transcript'], model.bert, bert_tokenizer, device)\n",
        "    train_df['post_training_embeds'] = [mean_embed_dict[sent] for sent in train_df['clean_transcript']]\n",
        "    test_df['post_training_embeds'] = [mean_embed_dict[sent] for sent in test_df['clean_transcript']]\n",
        "    df['post_training_embeds'] = [mean_embed_dict[sent] for sent in df['clean_transcript']]\n",
        "        \n",
        "    y_pred_ta_split = eval_utils.get_bert_n_closest_score(train_df,\n",
        "                                                          test_df,\n",
        "                                                          \"post_training_embeds\",\n",
        "                                                          criterion_column)\n",
        "    all_predictions_knn+=y_pred_ta_split\n",
        "\n",
        "  \n",
        "  if e==0:\n",
        "    results_df['samples']=all_samples\n",
        "    results_df['true_bins']=all_true_cls\n",
        "    results_df['true_scores']=all_true_reg\n",
        "    results_df['epoch0_bins']=all_predictions_cls\n",
        "    results_df['epoch0_scores']=all_predictions_knn\n",
        "  else:\n",
        "    results_df['epoch'+str(e)+\"_bins\"]=all_predictions_cls\n",
        "    results_df['epoch'+str(e)+\"_scores\"]=all_predictions_knn\n",
        "  \n",
        "  print(\"POST TRAINING CLS\")\n",
        "  eval_utils.evaluate_cls([x+1 for x in all_true_cls], [x+1 for x in all_predictions_cls])\n",
        "  eval_utils.evaluate_reg([x+1 for x in all_true_cls], [x+1 for x in all_predictions_cls], \"sbert mean cls epoch {}\".format(e))\n",
        "  \n",
        "  print(\"POST TRAINING 1NN\")\n",
        "  eval_utils.evaluate_cls(all_true_reg, all_predictions_knn)\n",
        "  print('----------')\n",
        "  eval_utils.evaluate_reg(all_true_reg, all_predictions_knn, \"sbert mean 1nn epoch {}\".format(e))\n",
        "  print('----------')\n",
        "  eval_utils.compute_bin_scores(df, 'post_training_embeds', bin_column)\n",
        "  eval_utils.plot_n_random_tasks(df, 'task', 'post_training_embeds', n=10)\n",
        "  eval_utils.plot_subtask(df, 'task', 1, 'post_training_embeds', criterion_column)\n",
        "  #-----------------------------------------\n",
        "\n",
        "  results_df.to_csv(\"drive/MyDrive/\"+folder_name+\"/results_cls.csv\", index=False)"
      ],
      "metadata": {
        "id": "2kMUYj0OFeVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}