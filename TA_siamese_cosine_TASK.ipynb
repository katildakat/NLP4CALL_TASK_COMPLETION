{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laaCqpI2S_hf",
        "outputId": "651d1448-46b0-4984-b6cd-51214ab5537a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q umap\n",
        "!pip install -q umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJMCWXGPPrc3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import glob\n",
        "import itertools\n",
        "from datasets import Dataset\n",
        "from scipy import stats\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "from sentence_transformers import models, SentenceTransformer, losses, evaluation, InputExample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QRKci_FKWAz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "import eval_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZVjQWzIG6hK"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYKS4QLp_REu"
      },
      "outputs": [],
      "source": [
        "def make_cls_pairs_cosine_similarity(df, score_column, transcript_column, score_interval=[1,3], tasks_to_drop=[]):\n",
        "    train_examples = []\n",
        "    processed_pairs = set()\n",
        "    \n",
        "    df=df[~df['task'].isin(tasks_to_drop)]\n",
        "    for i, row in df.iterrows():\n",
        "        # Get sample\n",
        "        sample = row[transcript_column]\n",
        "        sample_task = row['task']\n",
        "        sample_score = row[score_column]\n",
        "        \n",
        "        # Get pairs within the same task\n",
        "        task_df = df[(df['task'] == sample_task) & (df[transcript_column] != sample)].copy()\n",
        "        \n",
        "        for j, pair_row in task_df.iterrows():\n",
        "            pair_sample = pair_row[transcript_column]\n",
        "            \n",
        "            # Check if the pair has already been processed\n",
        "            if (sample, pair_sample) in processed_pairs or (pair_sample, sample) in processed_pairs:\n",
        "                continue\n",
        "\n",
        "            pair_score = pair_row[score_column]\n",
        "            \n",
        "            # Calculate cosine similarity label based on scores\n",
        "            score_diff = abs(sample_score - pair_score)\n",
        "            max_diff = interval[1] - interval[0]\n",
        "            cosine_similarity_label = 1.0 - (score_diff / max_diff) \n",
        "            \n",
        "            train_examples.append(InputExample(texts=[sample, pair_sample], label=cosine_similarity_label))\n",
        "\n",
        "            # Add the processed pair to the set\n",
        "            processed_pairs.add((sample, pair_sample))\n",
        "    \n",
        "    return train_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kMUYj0OFeVz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"drive/MyDrive/swedish_average.csv\")\n",
        "range_min = int(df['cefr_mean'].min())\n",
        "range_max = int(df['cefr_mean'].max())\n",
        "interval = [range_min, range_max]\n",
        "num_bins = range_max-range_min+1\n",
        "df['cefr_bins'] = eval_utils.get_hist_bin(df['cefr_mean'].tolist(), range_min, range_max)\n",
        "\n",
        "\n",
        "df['cefr_round'] = [eval_utils.school_round(x) for x in df['cefr_mean']]\n",
        "df = df[~df['cefr_round'].isin([1,6])]\n",
        "\n",
        "\n",
        "model_name = \"drive/MyDrive/SWE_TASK_MODELS_SIAM/epoch4_split{}\"\n",
        "\n",
        "tasks_to_drop = []\n",
        "#tasks_to_drop = [1,13,3]\n",
        "#tasks_to_drop = [23, 27, 15]\n",
        "\n",
        "criterion_column = 'cefr_mean' #'ta_facets'\n",
        "bin_column = 'cefr_round' #'ta_bins_r'\n",
        "folder_name = \"SWE_CEFR_MODELS_SIAM_COSINE_TASK\"\n",
        "\n",
        "results_df = pd.DataFrame()\n",
        "all_true = []\n",
        "all_samples = []\n",
        "\n",
        "for e in range(5):\n",
        "  print(\"epoch \"+str(e))\n",
        "  all_predictions = []\n",
        "  pred_before_training = []\n",
        "  for split in df['split'].unique():\n",
        "    print(\"----------------------------\")\n",
        "    print(\"split {}\".format(split))\n",
        "    \n",
        "    # make saving path\n",
        "    model_path = \"drive/MyDrive/\"+folder_name+\"/epoch{}_split{}\".format(e,split)\n",
        "    \n",
        "    train_df = df[df['split']!=split].reset_index(drop=True)\n",
        "    test_df = df[df['split']==split].reset_index(drop=True)\n",
        "\n",
        "    if e == 0:\n",
        "      # add values to the df\n",
        "      split_true = test_df[criterion_column].tolist()\n",
        "      all_true+=split_true\n",
        "      \n",
        "      split_sample = test_df['sample'].tolist()\n",
        "      all_samples+=split_sample\n",
        "\n",
        "      # load untrained model\n",
        "      model = SentenceTransformer(model_name.format(split), device=device)\n",
        "\n",
        "      pre_emb_dict = eval_utils.get_embed_dict(df['clean_transcript'].unique().tolist(), model)\n",
        "      train_df['pre_training_embeds'] = [pre_emb_dict[sent] for sent in train_df['clean_transcript']]\n",
        "      test_df['pre_training_embeds'] = [pre_emb_dict[sent] for sent in test_df['clean_transcript']]\n",
        "      df['pre_training_embeds'] = [pre_emb_dict[sent] for sent in df['clean_transcript']]\n",
        "\n",
        "          \n",
        "      y_pred_ta = eval_utils.get_bert_n_closest_score(train_df,\n",
        "                                                      test_df,\n",
        "                                                      \"pre_training_embeds\",\n",
        "                                                      criterion_column)\n",
        "      pred_before_training+=y_pred_ta\n",
        "      \n",
        "    else:\n",
        "      # load trained in previous epoch\n",
        "      pre_model_path = \"drive/MyDrive/\"+folder_name+\"/epoch{}_split{}\".format(e-1,split)\n",
        "      model = SentenceTransformer(pre_model_path, device=device)\n",
        "    \n",
        "    emb_dict = eval_utils.get_embed_dict(train_df['clean_transcript'].unique().tolist(), model)\n",
        "    train_df['pre_training_embeds'] = [emb_dict[sent] for sent in train_df['clean_transcript']]\n",
        "    train_examples = make_cls_pairs_cosine_similarity(train_df, criterion_column, \"clean_transcript\", score_interval=interval)\n",
        "    #random.shuffle(train_examples)\n",
        "    #train_examples = train_examples[:200]\n",
        "\n",
        "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "    \n",
        "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=50)\n",
        "    model.save(model_path)\n",
        "    \n",
        "    emb_dict = eval_utils.get_embed_dict(df['clean_transcript'].unique().tolist(), model)\n",
        "    train_df['post_training_embeds'] = [emb_dict[sent] for sent in train_df['clean_transcript']]\n",
        "    test_df['post_training_embeds'] = [emb_dict[sent] for sent in test_df['clean_transcript']]\n",
        "    df['post_training_embeds'] = [emb_dict[sent] for sent in df['clean_transcript']]\n",
        "    \n",
        "    y_pred_ta = eval_utils.get_bert_n_closest_score(train_df,\n",
        "                                                    test_df,\n",
        "                                                    \"post_training_embeds\",\n",
        "                                                    criterion_column)\n",
        "\n",
        "    all_predictions+=y_pred_ta\n",
        "\n",
        "  \n",
        "  if e==0:\n",
        "    results_df['samples']=all_samples\n",
        "    results_df['true']=all_true\n",
        "    results_df['pre']=pred_before_training\n",
        "    results_df['epoch0']=all_predictions\n",
        "\n",
        "    print(\"PRE TRAINING 1NN\")\n",
        "    eval_utils.evaluate_cls(all_true, pred_before_training)\n",
        "    print('----------')\n",
        "    eval_utils.evaluate_reg(all_true, pred_before_training, \"pre\")\n",
        "    eval_utils.compute_bin_scores(df, 'pre_training_embeds', bin_column)\n",
        "    eval_utils.plot_subtask(df, 'task', 1, 'pre_training_embeds', criterion_column)\n",
        "\n",
        "\n",
        "  else:\n",
        "    results_df['epoch'+str(e)]=all_predictions\n",
        "  \n",
        "  print(\"POST TRAINING 1NN\")\n",
        "  eval_utils.evaluate_cls(all_true, all_predictions)\n",
        "  print('----------')\n",
        "  eval_utils.evaluate_reg(all_true, all_predictions, \"sbert 1nn epoch {}\".format(e))\n",
        "  print('----------')\n",
        "  eval_utils.compute_task_scores(df, 'task','post_training_embeds')\n",
        "  print('----------')\n",
        "  eval_utils.compute_bin_scores(df, 'post_training_embeds', bin_column)\n",
        "  eval_utils.plot_n_random_tasks(df, 'task', 'post_training_embeds', n=10)\n",
        "  eval_utils.plot_subtask(df, 'task', 1, 'post_training_embeds', criterion_column)\n",
        "  #-----------------------------------------\n",
        "\n",
        "  results_df.to_csv(\"drive/MyDrive/\"+folder_name+\"/results_cls.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}